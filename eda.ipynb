{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd89012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sgp4.api import Satrec\n",
    "from sgp4.conveniences import sat_epoch_datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import curve_fit, root_scalar\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10161f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, TimeDistributed, Dense\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924c194",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ada17",
   "metadata": {},
   "outputs": [],
   "source": [
    "decayed_df = pd.read_csv(\"data/tip.csv\")\n",
    "decayed_df['MSG_EPOCH'] = pd.to_datetime(decayed_df['MSG_EPOCH'], format='%Y-%m-%d %H:%M:%S')\n",
    "decayed_df['DECAY_EPOCH'] = pd.to_datetime(decayed_df['DECAY_EPOCH'], format='%Y-%m-%d %H:%M:%S')\n",
    "decayed_df = decayed_df.sort_values(by='MSG_EPOCH', ascending=False)\n",
    "decayed_df = decayed_df.drop_duplicates(subset='NORAD_CAT_ID', keep='first')\n",
    "decayed_df = decayed_df.sort_values(by='NORAD_CAT_ID').reset_index(drop=True)\n",
    "decayed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decayed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e85737",
   "metadata": {},
   "outputs": [],
   "source": [
    "decayed_list = decayed_df['NORAD_CAT_ID'].to_list()\n",
    "decayed_tip = decayed_df['DECAY_EPOCH'].to_list()\n",
    "len(decayed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e1679f",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f85d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "decayed_df['RCS_SIZE'].value_counts().sort_index().plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Formatting\n",
    "plt.title('Distribution of RCS_SIZE')\n",
    "plt.xlabel('RCS_SIZE Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19289f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tle_file(tle_path):\n",
    "    tles = []\n",
    "    with open(tle_path) as f:\n",
    "        lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "    for i in range(0, len(lines), 2):\n",
    "        # name = lines[i]\n",
    "        line1 = lines[i]\n",
    "        line2 = lines[i+1]\n",
    "        tles.append((line1, line2))\n",
    "    return tles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(tles):\n",
    "    df = pd.DataFrame(columns=['EPOCH_DATE_TIME', 'ECC', 'B*', 'perigee', 'a'])\n",
    "    for s, t in tles:\n",
    "        satellite = Satrec.twoline2rv(s, t)\n",
    "        df.loc[len(df)] = [sat_epoch_datetime(satellite), satellite.ecco, satellite.bstar, satellite.altp * satellite.radiusearthkm, satellite.a * satellite.radiusearthkm]\n",
    "    df.set_index('EPOCH_DATE_TIME', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    # df.sort_values('EPOCH_DATE_TIME', inplace=True)\n",
    "    return df\n",
    "\n",
    "def generate_df_altitude(tles):\n",
    "    df = pd.DataFrame(columns=['EPOCH_DATE_TIME', 'altitude'])\n",
    "    for s, t in tles:\n",
    "        satellite = Satrec.twoline2rv(s, t)\n",
    "        df.loc[len(df)] = [sat_epoch_datetime(satellite), satellite.altp * satellite.radiusearthkm]\n",
    "    df.set_index('EPOCH_DATE_TIME', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df = df[df['altitude'] <= 200]\n",
    "    if len(df) != 0:\n",
    "        df = df[df['altitude'].idxmax() : ]\n",
    "        df['hours'] = (df.index - df.index[0]).total_seconds() / 3600\n",
    "    return df\n",
    "\n",
    "def generate_df_altitude_with_tip(tles, time, alt=10):\n",
    "    df = pd.DataFrame(columns=['EPOCH_DATE_TIME', 'altitude'])\n",
    "    for idx, (s, t) in enumerate(tles):\n",
    "        satellite = Satrec.twoline2rv(s, t)\n",
    "        df.loc[len(df)] = [sat_epoch_datetime(satellite), satellite.altp * satellite.radiusearthkm]\n",
    "    df.loc[len(df)] = [time.to_pydatetime() , alt]\n",
    "    df['EPOCH_DATE_TIME'] = pd.to_datetime(df['EPOCH_DATE_TIME'], utc=True)\n",
    "    df.sort_values(by=['EPOCH_DATE_TIME'])\n",
    "    df = df[df['altitude'] <= 250]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bstar = []\n",
    "ecc = []\n",
    "missedFiles = []\n",
    "for id in decayed_list:\n",
    "    try:\n",
    "        tles = read_tle_file(f'objects/{id}.txt')\n",
    "        df = generate_df(tles)\n",
    "        trimmed = df[df['perigee'] <= 200]\n",
    "        if len(trimmed) != 0:\n",
    "            trimmed = trimmed[trimmed['perigee'].idxmax() : ]\n",
    "            if len(trimmed) != 0:\n",
    "                bstar.extend(trimmed['B*'].to_list())\n",
    "                ecc.extend(trimmed['ECC'].to_list())\n",
    "    except:\n",
    "        missedFiles.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecc_series = pd.Series(ecc, name='Eccentricity')\n",
    "bstar_series = pd.Series(bstar, name='B*')\n",
    "\n",
    "# Plot Eccentricity\n",
    "plt.figure(figsize=(10, 4))\n",
    "ecc_series.plot(kind='hist', bins=50, color='orange', edgecolor='black')\n",
    "plt.title('Distribution of Eccentricity (<=200km Perigee)')\n",
    "plt.xlabel('Eccentricity')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot B*\n",
    "plt.figure(figsize=(10, 4))\n",
    "bstar_series.plot(kind='hist',bins=50, color='red', edgecolor='black')\n",
    "plt.title('Distribution of B* (<=200km Perigee)')\n",
    "plt.xlabel('B*')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00e3ae",
   "metadata": {},
   "source": [
    "#### Curve Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ba082",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_decayed_list = []\n",
    "valid_decayed_tips = []\n",
    "missing_list = []\n",
    "for id, tip in zip(decayed_list, decayed_tip):\n",
    "    try:\n",
    "        with open(f'objects/{id}.txt') as f:\n",
    "            lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "            valid_decayed_list.append(id)\n",
    "            valid_decayed_tips.append(tip)\n",
    "    except:\n",
    "        missing_list.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d414de",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 5\n",
    "thresh = 3\n",
    "al = df['altitude'].iloc[:-1]\n",
    "result = seasonal_decompose(al, model='additive',extrapolate_trend='freq', period=3)\n",
    "residuals = pd.Series(result.resid)\n",
    "mean = residuals.mean()\n",
    "std = residuals.std()\n",
    "threshold_upper = mean + thresh * std  # 3 standard deviations above\n",
    "threshold_lower = mean - thresh * std  # 3 standard deviations below\n",
    "major_shifts = residuals[(residuals > threshold_upper) | (residuals < threshold_lower)]\n",
    "\n",
    "indexes = list(filter(lambda x: x != df.index[-2], major_shifts.index))\n",
    "if len(indexes) != 0:\n",
    "    tdf = df.drop(index=indexes)\n",
    "else:\n",
    "    tdf = df.copy()\n",
    "tdf['time_hours'] = (tdf['EPOCH_DATE_TIME'] - tdf['EPOCH_DATE_TIME'].min()).dt.total_seconds() / 3600\n",
    "\n",
    "tref = tdf['time_hours'].iloc[-1]\n",
    "\n",
    "def model(t, *params):\n",
    "    a1 = params[0]\n",
    "    result = a1\n",
    "    for k in range(2, max_k + 2):\n",
    "        result += params[k - 1] * (tref-t) ** (1 / k)\n",
    "    return result\n",
    "\n",
    "t_data = tdf['time_hours'].values\n",
    "y_data = tdf['altitude'].values\n",
    "\n",
    "initial_guess = np.ones(max_k + 1)\n",
    "    \n",
    "params, covariance = curve_fit(model, t_data, y_data, p0=initial_guess)\n",
    "    \n",
    "# a1_opt, *ak_opt = params\n",
    "# step = 1/60\n",
    "# num = int(np.floor((t_data.max() - t_data.min()) / step)) + 1\n",
    "# t_fit = np.linspace(t_data.min(), t_data.max(), num)\n",
    "# y_fit = model(t_fit, *params)\n",
    "\n",
    "def predict_times_for_altitudes(h_targets, params, t_bounds):\n",
    "    predicted_times = []\n",
    "    for h_target in h_targets:\n",
    "        def func(t):\n",
    "            return model(t, *params) - h_target\n",
    "\n",
    "        f_a = func(t_bounds[0])\n",
    "        f_b = func(t_bounds[1])\n",
    "\n",
    "        if f_a * f_b > 0:\n",
    "            # No sign change: root not guaranteed in the interval\n",
    "            predicted_times.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = root_scalar(func, bracket=t_bounds, method='toms748')\n",
    "            if result.converged:\n",
    "                predicted_times.append(result.root)\n",
    "            else:\n",
    "                predicted_times.append(np.nan)\n",
    "        except ValueError:\n",
    "            predicted_times.append(np.nan)\n",
    "    return predicted_times\n",
    "\n",
    "target_altitudes = np.linspace(200, 10, 25)\n",
    "t_bounds = (t_data[0], t_data[-1])\n",
    "times_to_targets = predict_times_for_altitudes(target_altitudes, params, t_bounds)\n",
    "times_to_targets = np.array(times_to_targets)\n",
    "\n",
    "if np.count_nonzero(np.isnan(times_to_targets)) == 1 and np.isnan(times_to_targets[-1]):\n",
    "    times_to_targets[-1] = tdf['time_hours'].iloc[-1]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "fdf = tdf[tdf['altitude'] <= 200]\n",
    "plt.scatter(fdf['time_hours'], fdf['altitude'],\n",
    "            color='blue', label='Data points')\n",
    "\n",
    "# plt.plot(t_fit, y_fit, color='red', linestyle='-', marker='o',\n",
    "#          label=f'fit (degree={max_k})')\n",
    "\n",
    "\n",
    "plt.plot(times_to_targets, target_altitudes, color='green', linestyle='--', marker='o',\n",
    "         label=f'fit')\n",
    "\n",
    "plt.xlabel('hours')\n",
    "plt.ylabel('altitude')\n",
    "plt.title('Fit: Epoch vs. Mean Perigee')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in valid_decayed_list:\n",
    "    source = f'objects/{id}.txt'\n",
    "    destination = f'objects_dec/{id}.txt'\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(max_k=5):\n",
    "    z = 0\n",
    "    tip_alt = 10\n",
    "    invalid_dataset = []\n",
    "    for id, tip in zip(valid_decayed_list[z:], valid_decayed_tips[z:]):\n",
    "        try:\n",
    "            tles = read_tle_file(f'objects/{id}.txt')\n",
    "            df = pd.DataFrame(columns=['EPOCH_DATE_TIME', 'altitude'])\n",
    "            for idx, (s, t) in enumerate(tles):\n",
    "                satellite = Satrec.twoline2rv(s, t)\n",
    "                df.loc[len(df)] = [sat_epoch_datetime(satellite), satellite.altp * satellite.radiusearthkm]\n",
    "            df.loc[len(df)] = [tip.to_pydatetime() , tip_alt]\n",
    "            df['EPOCH_DATE_TIME'] = pd.to_datetime(df['EPOCH_DATE_TIME'], utc=True)\n",
    "            df.sort_values(by=['EPOCH_DATE_TIME'],inplace=True)\n",
    "            df = df[df['altitude'] <= 250]\n",
    "\n",
    "            if len(df) < 10:\n",
    "                invalid_dataset.append(id)\n",
    "                continue\n",
    "                \n",
    "            df['epoch_ts'] = df['EPOCH_DATE_TIME'].apply(lambda dt: dt.timestamp())\n",
    "\n",
    "            thresh = 3\n",
    "            al = df['altitude'].iloc[:-1]\n",
    "            result = seasonal_decompose(al, model='additive',extrapolate_trend='freq', period=3)\n",
    "            residuals = pd.Series(result.resid)\n",
    "            mean = residuals.mean()\n",
    "            std = residuals.std()\n",
    "            threshold_upper = mean + thresh * std  # 3 standard deviations above\n",
    "            threshold_lower = mean - thresh * std  # 3 standard deviations below\n",
    "            major_shifts = residuals[(residuals > threshold_upper) | (residuals < threshold_lower)]\n",
    "            indexes = list(filter(lambda x: x != df.index[-2], major_shifts.index))\n",
    "            if len(indexes) != 0:\n",
    "                tdf = df.drop(index=indexes)\n",
    "            else:\n",
    "                tdf = df.copy()\n",
    "            tdf['time_hours'] = (tdf['EPOCH_DATE_TIME'] - tdf['EPOCH_DATE_TIME'].min()).dt.total_seconds() / 3600\n",
    "            tref = tdf['time_hours'].iloc[-1]\n",
    "            \n",
    "            def model(t, *params):\n",
    "                a1 = params[0]\n",
    "                result = a1\n",
    "                for k in range(2, max_k + 2):\n",
    "                    result += params[k - 1] * (tref-t) ** (1 / k)\n",
    "                return result\n",
    "\n",
    "            t_data = tdf['time_hours'].values\n",
    "            y_data = tdf['altitude'].values\n",
    "\n",
    "            initial_guess = np.ones(max_k + 1)\n",
    "                \n",
    "            params, covariance = curve_fit(model, t_data, y_data, p0=initial_guess)\n",
    "\n",
    "            def predict_times_for_altitudes(h_targets, params, t_bounds):\n",
    "                predicted_times = []\n",
    "                for h_target in h_targets:\n",
    "                    def func(t):\n",
    "                        return model(t, *params) - h_target\n",
    "\n",
    "                    f_a = func(t_bounds[0])\n",
    "                    f_b = func(t_bounds[1])\n",
    "\n",
    "                    if f_a * f_b > 0:\n",
    "                        # No sign change: root not guaranteed in the interval\n",
    "                        predicted_times.append(np.nan)\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        result = root_scalar(func, bracket=t_bounds, method='brentq')\n",
    "                        if result.converged:\n",
    "                            predicted_times.append(result.root)\n",
    "                        else:\n",
    "                            predicted_times.append(np.nan)\n",
    "                    except ValueError:\n",
    "                        predicted_times.append(np.nan)\n",
    "                return predicted_times\n",
    "            \n",
    "            \n",
    "            target_altitudes = np.linspace(200, 80,25)\n",
    "            t_bounds = (t_data[0], t_data[-1])\n",
    "            times_to_targets = predict_times_for_altitudes(target_altitudes, params, t_bounds)\n",
    "            times_to_targets = np.array(times_to_targets)\n",
    "\n",
    "            if np.count_nonzero(np.isnan(times_to_targets)) == 1 and np.isnan(times_to_targets[-1]):\n",
    "                times_to_targets[-1] = tdf['time_hours'].iloc[-1]\n",
    "            elif np.count_nonzero(np.isnan(times_to_targets)) > 1:\n",
    "                invalid_dataset.append(id)\n",
    "                continue\n",
    "\n",
    "            final_df = pd.DataFrame(columns=['edt', 'altitude'])\n",
    "            t0 = tdf['EPOCH_DATE_TIME'].min()\n",
    "            for x,y in zip(target_altitudes, times_to_targets):\n",
    "                final_df.loc[len(final_df)] = [t0 + pd.to_timedelta(y, unit='h'), x]\n",
    "            \n",
    "            final_df.sort_values(by=['edt'], inplace=True)\n",
    "            final_df['alt_diff'] = final_df['altitude'].diff()\n",
    "            if (final_df['alt_diff'].iloc[1:] > 0).any():\n",
    "                # print(\"Altitude is NOT strictly decreasing: there is at least one upward jump.\")\n",
    "                # # If you want to see exactly where:\n",
    "                bad_idxs = df.index[df['alt_diff'] > 0].tolist()\n",
    "                print(\"↑ Violations at row indices:\", bad_idxs)\n",
    "                print(\"↑ Violations at row values:\", df.loc[bad_idxs, 'alt_diff'])\n",
    "                invalid_dataset.append(id)\n",
    "                continue\n",
    "            else:\n",
    "                final_df = final_df.drop('alt_diff', axis=1)\n",
    "                final_df.to_csv(f'datasets/{id}.csv', index=False, date_format='%Y-%m-%d %H:%M:%S.%f%z')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(id)\n",
    "            invalid_dataset.append(id)\n",
    "\n",
    "    return invalid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_dataset = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(invalid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0505f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0\n",
    "m = 0\n",
    "z = 0\n",
    "for id, tip in zip(valid_decayed_list[z:], valid_decayed_tips[z:]):\n",
    "    tles = read_tle_file(f'objects/{id}.txt')\n",
    "    df = generate_df_altitude_with_tip(tles, tip)\n",
    "    df = df.reset_index(drop=True)\n",
    "    if len(df) <= 3:\n",
    "        m += 1\n",
    "    else:\n",
    "        v += 1\n",
    "    print(f\"{v+m} / {len(valid_decayed_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_SEQ_LEN = 15\n",
    "# OUTPUT_SEQ_LEN = 10\n",
    "# path = 'datasets/60.csv'\n",
    "# df = pd.read_csv(path)\n",
    "\n",
    "# df['edt'] = pd.to_datetime(df['edt'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "# df['hours'] = (df['edt'] - df['edt'].iloc[0]).dt.total_seconds() / 3600.0\n",
    "# hours = df['hours'].values.astype(np.float32)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# norm_hours = scaler.fit_transform(hours.reshape(-1,1)).flatten()\n",
    "\n",
    "# encoder_inputs, decoder_inputs, decoder_targets = [], [], []\n",
    "\n",
    "# encoder_inputs.append(norm_hours[:INPUT_SEQ_LEN].reshape(-1, 1))\n",
    "# di = norm_hours[INPUT_SEQ_LEN - 1:-1].copy()\n",
    "# di[0] = 0.0\n",
    "# decoder_inputs.append(di.reshape(-1, 1))\n",
    "# decoder_targets.append(norm_hours[INPUT_SEQ_LEN:].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "INPUT_SEQ_LEN = 15\n",
    "OUTPUT_SEQ_LEN = 10\n",
    "\n",
    "# Data generator with decoder inputs\n",
    "class SequenceGenerator(Sequence):\n",
    "    def __init__(self, file_paths, batch_size=32):\n",
    "        self.file_paths = file_paths\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.file_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_paths = self.file_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        encoder_inputs, decoder_inputs, decoder_targets = [], [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            df = pd.read_csv(path)\n",
    "            if len(df) != INPUT_SEQ_LEN + OUTPUT_SEQ_LEN:\n",
    "                continue\n",
    "\n",
    "            df['edt'] = pd.to_datetime(df['edt'], format='%Y-%m-%d %H:%M:%S.%f%z')\n",
    "            df['hours'] = (df['edt'] - df['edt'].iloc[0]).dt.total_seconds() / 3600.0\n",
    "            hours = df['hours'].values.astype(np.float32)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            norm_hours = scaler.fit_transform(hours.reshape(-1,1)).flatten()\n",
    "\n",
    "            encoder_inputs.append(norm_hours[:INPUT_SEQ_LEN].reshape(-1, 1))\n",
    "            decoder_inputs.append(norm_hours[INPUT_SEQ_LEN - 1:-1].reshape(-1, 1))\n",
    "            decoder_targets.append(norm_hours[INPUT_SEQ_LEN:].reshape(-1, 1))\n",
    "\n",
    "\n",
    "            # encoder_inputs.append(hours[:INPUT_SEQ_LEN].reshape(-1, 1))\n",
    "            # decoder_inputs.append(hours[INPUT_SEQ_LEN - 1:-1].reshape(-1, 1))\n",
    "            # decoder_targets.append(hours[INPUT_SEQ_LEN:].reshape(-1, 1))\n",
    "\n",
    "        return [np.array(encoder_inputs), np.array(decoder_inputs)], np.array(decoder_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq2seq_gru(latent_dim=64):\n",
    "    encoder_inputs = Input(shape=(INPUT_SEQ_LEN, 1), name='encoder_inputs')\n",
    "    encoder_gru1 = GRU(latent_dim, return_sequences=True, name='encoder_gru_1')\n",
    "    encoder_seq1 = encoder_gru1(encoder_inputs)\n",
    "\n",
    "    encoder_gru2 = GRU(latent_dim, return_sequences=True, name='encoder_gru_2')\n",
    "    encoder_seq2 = encoder_gru2(encoder_seq1)\n",
    "\n",
    "    encoder_gru3 = GRU(latent_dim, return_sequences=True, name='encoder_gru_3')\n",
    "    encoder_seq3 = encoder_gru3(encoder_seq2)\n",
    "\n",
    "    encoder_gru4 = GRU(latent_dim, return_sequences=True, name='encoder_gru_4')\n",
    "    encoder_seq4 = encoder_gru4(encoder_seq3)\n",
    "\n",
    "    # encoder_gru5 = GRU(latent_dim, return_sequences=True, name='encoder_gru_5')\n",
    "    # encoder_seq5 = encoder_gru5(encoder_seq4)\n",
    "\n",
    "    # encoder_gru6 = GRU(latent_dim, return_sequences=True, name='encoder_gru_6')\n",
    "    # encoder_seq6 = encoder_gru6(encoder_seq5)\n",
    "\n",
    "    # encoder_gru7 = GRU(latent_dim, return_sequences=True, name='encoder_gru_7')\n",
    "    # encoder_seq7 = encoder_gru7(encoder_seq6)\n",
    "\n",
    "    encoder_gru8 = GRU(latent_dim, return_state=True, name='encoder_gru_8')\n",
    "    _, state_h = encoder_gru8(encoder_seq4)\n",
    "\n",
    "    decoder_inputs = Input(shape=(OUTPUT_SEQ_LEN, 1), name='decoder_inputs')\n",
    "    decoder_gru1 = GRU(latent_dim, return_sequences=True, name='decoder_gru_1')\n",
    "    decoder_seq1 = decoder_gru1(decoder_inputs, initial_state=state_h)\n",
    "\n",
    "    decoder_gru2 = GRU(latent_dim, return_sequences=True, name='decoder_gru_2')\n",
    "    decoder_seq2 = decoder_gru2(decoder_seq1)\n",
    "\n",
    "    decoder_gru3 = GRU(latent_dim, return_sequences=True, name='decoder_gru_3')\n",
    "    decoder_seq3 = decoder_gru3(decoder_seq2)\n",
    "\n",
    "    decoder_gru4 = GRU(latent_dim, return_sequences=True, name='decoder_gru_4')\n",
    "    decoder_seq4 = decoder_gru4(decoder_seq3)\n",
    "\n",
    "    decoder_gru5 = GRU(latent_dim, return_sequences=True, name='decoder_gru_5')\n",
    "    decoder_seq5 = decoder_gru5(decoder_seq4)\n",
    "\n",
    "    # decoder_gru6 = GRU(latent_dim, return_sequences=True, name='decoder_gru_6')\n",
    "    # decoder_seq6 = decoder_gru6(decoder_seq5)\n",
    "\n",
    "    # decoder_gru7 = GRU(latent_dim, return_sequences=True, name='decoder_gru_7')\n",
    "    # decoder_seq7 = decoder_gru7(decoder_seq6)\n",
    "\n",
    "    # decoder_gru8 = GRU(latent_dim, return_sequences=True, name='decoder_gru_8')\n",
    "    # decoder_seq8 = decoder_gru8(decoder_seq7)\n",
    "\n",
    "    decoder_dense = TimeDistributed(Dense(1, name='decoder_dense_node_1'), name='decoder_dense_dist_1')\n",
    "    decoder_outputs = decoder_dense(decoder_seq5)\n",
    "\n",
    "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=[rmse, 'mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede21586",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'datasets'\n",
    "csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "train_files, test_files = train_test_split(\n",
    "        csv_files,\n",
    "        test_size=0.1,    \n",
    "        random_state=42,\n",
    ")\n",
    "batch_size = 32\n",
    "epochs = 2000\n",
    "\n",
    "train_gen = SequenceGenerator(train_files, batch_size=32)\n",
    "test_gen = SequenceGenerator(test_files, batch_size=32)\n",
    "\n",
    "print(f\"Training: {len(train_files)} files; Validation: {len(test_files)} files\")\n",
    "\n",
    "checkpoint_path = \"case_D/layers_5/neurons_100/gru/training_1_gru/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                    patience = 20, restore_best_weights = False)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "\n",
    "gru_model = build_seq2seq_gru(100)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e684bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = gru_model.fit(\n",
    "    train_gen,\n",
    "    validation_data = test_gen,\n",
    "    epochs=epochs,\n",
    "    callbacks=[ checkpoint, lr_scheduler, early_stop]\n",
    ")\n",
    "gru_model.save('case_D/layers_5/neurons_300/gru/seq2seq_altitude_gru', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 38086\n",
    "path = f\"datasets/{id}.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df['edt'] = pd.to_datetime(df['edt'], format='%Y-%m-%d %H:%M:%S.%f%z')\n",
    "df['hours'] = (df['edt'] - df['edt'].iloc[0]).dt.total_seconds() / 3600.0\n",
    "hours = df['hours'].values.astype(np.float32)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "norm_hours = scaler.fit_transform(hours.reshape(-1,1)).flatten()\n",
    "\n",
    "encoder_input = norm_hours[:INPUT_SEQ_LEN].reshape(1, INPUT_SEQ_LEN, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df['hours'], df['altitude'],\n",
    "            color='blue', label='Data points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b162d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa555b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = np.zeros((1, OUTPUT_SEQ_LEN, 1))\n",
    "decoder_input[0, 0, 0] = encoder_input[0, -1, 0]\n",
    "\n",
    "# Placeholder for predictions\n",
    "predictions = []\n",
    "\n",
    "for i in range(OUTPUT_SEQ_LEN):\n",
    "    # Predict the next value\n",
    "    output = gru_model.predict([encoder_input, decoder_input], verbose=0)\n",
    "    next_value = output[0, i, 0]\n",
    "    predictions.append(next_value)\n",
    "    \n",
    "    # Update the decoder input for the next time step\n",
    "    if i + 1 < OUTPUT_SEQ_LEN:\n",
    "        decoder_input[0, i + 1, 0] = next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d03e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "predicted_values = scaler.inverse_transform(predictions.reshape(-1,1))\n",
    "true_future = hours[INPUT_SEQ_LEN:] \n",
    "predictions_hr = predicted_values.flatten()\n",
    "rel_err_percent = np.full_like(true_future, np.nan, dtype=np.float32)\n",
    "mask_nonzero = true_future != 0\n",
    "rel_err_percent[mask_nonzero] = (\n",
    "    (predictions_hr[mask_nonzero] - true_future[mask_nonzero])\n",
    "    / true_future[mask_nonzero]\n",
    ") * 100.0\n",
    "abs_rel_err_percent = np.abs(rel_err_percent)\n",
    "mean_rel_err_percent      = np.nanmean(rel_err_percent) \n",
    "abs_error = np.abs(predictions_hr - true_future)  \n",
    "MAE_hours = abs_error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df['hours'], df['altitude'],\n",
    "            color='blue', label='Data points')\n",
    "\n",
    "plt.plot(predicted_values, df['altitude'].iloc[INPUT_SEQ_LEN:], color='red', linestyle='--', marker='o',\n",
    "         label='predicted')\n",
    "plt.plot([], [], ' ', label=f'Absolute Error (hours): {MAE_hours:.2f}')\n",
    "plt.plot([], [], ' ', label=f'Relative Error (%): {mean_rel_err_percent:.2f}')\n",
    "plt.xlabel('elapsed time (hours) from altitude = 200 km')\n",
    "plt.ylabel('altitude (km)')\n",
    "plt.title(f'Object {id} 5 layer 100 n')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
